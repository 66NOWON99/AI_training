{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b37328c5bd0e>, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-b37328c5bd0e>\"\u001b[1;36m, line \u001b[1;32m77\u001b[0m\n\u001b[1;33m    0 0.3000054\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#예시\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./concrete.csv\", sep=\",\", header=0)\n",
    "data\n",
    "\n",
    "def feture_scaling(df, scaling_strategy=\"min-max\", column=None):\n",
    "    if column == None:\n",
    "        column = [column_name for column_name in df.columns]\n",
    "    for column_name in column:\n",
    "        if scaling_strategy == \"min-max\":\n",
    "            df[column_name] = ( df[column_name] - df[column_name].min() ) /\\\n",
    "                            (df[column_name].max() - df[column_name].min()) \n",
    "        elif scaling_strategy == \"z-score\":\n",
    "            df[column_name] = ( df[column_name] - \\\n",
    "                               df[column_name].mean() ) /\\\n",
    "                            (df[column_name].std() )\n",
    "    return df\n",
    "\n",
    "feture_scaling(data,column=[\"cement\",\"slag\",\"ash\",\"water\",\"superplastic\",\"coarseagg\",\"fineagg\",\"age\",\"strength\"])\n",
    "data.head\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "x_data = data[[\"cement\",\"slag\",\"ash\",\"water\",\"superplastic\",\"coarseagg\",\"fineagg\",\"age\"]]\n",
    "print(x_data.shape)\n",
    "y_data = data[[\"strength\"]]\n",
    "print(y_data.shape)\n",
    "\n",
    "num_node = 10\n",
    "num_layer = 2\n",
    "learn_rate = 0.01\n",
    "\n",
    "W_ = []\n",
    "b_ = []\n",
    "layer_ = []\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([8,num_node]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([num_node]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W_.append(W1)\n",
    "b_.append(b1)\n",
    "layer_.append(layer1)\n",
    "\n",
    "for i in range(1, num_layer):\n",
    "    W_.append(tf.Variable(tf.random_normal([num_node,num_node]), name='weight'))\n",
    "    b_.append(tf.Variable(tf.random_normal([num_node]), name='bias'))\n",
    "    layer_.append(tf.sigmoid(tf.matmul(layer_[i-1], W_[i]) + b_[i]))\n",
    "\n",
    "W_last = tf.Variable(tf.random_normal([num_node,1]), name='weight')\n",
    "b_last = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "Y_hat = tf.sigmoid(tf.matmul(layer_[-1], W_last) + b_last)\n",
    "loss = tf.reduce_mean(tf.square(Y - Y_hat))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learn_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_history = []\n",
    "    for step in range(10001):\n",
    "        cost_val, hypothesis, _ = sess.run([loss, Y_hat, train], feed_dict={X:x_data, Y:y_data}) # ì—°ì‚°ê·¸ëž˜í”„ ì‹¤í–‰í•  ë•Œ ë³€ìˆ˜ ê°’ì„ ë„£ìŒ\n",
    "        cost_history.append(cost_val)\n",
    "        if step % 2000 == 0:\n",
    "            print(step, cost_val)\n",
    "    plt.plot(cost_history)\n",
    "    plt.show()\n",
    "\n",
    "    pred = sess.run(Y_hat, feed_dict={X:x_data})\n",
    "    print(np.cov(np.array(y_data).reshape(-1), np.array(pred).reshape))\n",
    "#result\n",
    "# (1030, 8)\n",
    "# (1030, 1)\n",
    "# 0 0.3000054\n",
    "# 2000 0.04737836\n",
    "# 4000 0.04028104\n",
    "# 6000 0.03553781\n",
    "# 8000 0.03276191\n",
    "# 10000 0.030917015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분산이 작을수록 평균값(예측값)과 가까움  \n",
    "공분산 covariance  \n",
    "공분산 값이 커지면 분산도 커지고... cov  \n",
    "공분산이 작을수록 예측값과 분산이 가까워짐  \n",
    "minmax normalization (0,1)사이 구간 (마이너스가 의미가 없음/약해짐->(-)또한 0~1사이 구간으로 옮겨지기 때문)  \n",
    "z- score standardization 마이너스가 의미가 있는 데이터 (입/출, 방향, 재무구조 등)  \n",
    "\n",
    "ML은 정규화가 필수  \n",
    "z-score standardization  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.27450982 0.6117647  1.         0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.7607844  0.09411766 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.06666667 0.5764706  0.9803922  0.9607844\n",
      " 0.68235296 0.6784314  0.7960785  0.92549026 0.9921569  0.9960785\n",
      " 0.4784314  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.07450981\n",
      " 0.7686275  0.9921569  0.8235295  0.27450982 0.         0.\n",
      " 0.         0.04705883 0.5686275  0.9960785  0.5294118  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.0627451  0.78823537 0.9803922  0.7176471\n",
      " 0.0627451  0.         0.         0.         0.         0.08627451\n",
      " 0.8588236  0.9960785  0.3254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.0627451\n",
      " 0.76470596 0.9960785  0.61960787 0.         0.         0.\n",
      " 0.         0.         0.         0.59607846 0.9921569  0.9960785\n",
      " 0.15294118 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3921569  0.9960785  0.7686275\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.40000004 0.9960785  0.9960785  0.5882353  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.53333336 0.9921569  0.4431373  0.         0.\n",
      " 0.         0.         0.0509804  0.5764706  0.9450981  0.9921569\n",
      " 0.9921569  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.53333336\n",
      " 0.9921569  0.7176471  0.16470589 0.         0.16862746 0.30980393\n",
      " 0.87843144 0.9921569  0.9921569  0.9921569  0.7254902  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.4784314  0.9921569  0.9960785\n",
      " 0.9568628  0.9176471  0.9568628  0.9921569  0.75294125 0.7019608\n",
      " 0.9921569  0.9058824  0.13333334 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03529412 0.6666667  0.9960785  0.9921569  0.9921569\n",
      " 0.7843138  0.3137255  0.         0.6039216  0.9921569  0.8352942\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.09019608 0.20784315 0.         0.         0.\n",
      " 0.23137257 0.93725497 0.9960785  0.37254903 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.78823537 0.9921569\n",
      " 0.7294118  0.01176471 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.12156864 0.9960785  0.9921569  0.41960788 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.7372549\n",
      " 0.9960785  0.7137255  0.02352941 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.12156864 0.93725497 0.9960785  0.5294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.62352943 0.9960785  0.854902   0.05882353 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.02745098 0.9215687  0.9921569\n",
      " 0.4431373  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.427451   0.9921569  0.9921569  0.38431376 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07450981 0.9686275\n",
      " 0.9921569  0.9294118  0.1764706  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07843138 0.9921569  0.9921569  0.3137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "(array([9], dtype=int64),)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([8], dtype=int64),)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1], dtype=int64),)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2], dtype=int64),)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1], dtype=int64),)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#images <실제적 데이터, y<label\n",
    "#data 28x28 \n",
    "#accuracy ~ test data : mnist.test.images[i]\n",
    "#softmax& NN\n",
    "print(mnist.train.images[0])\n",
    "\n",
    "for i in range(5):\n",
    "#     print(mnist.train.labels[i])\n",
    "    print(np.where(mnist.train.labels[i] == 1))\n",
    "    plt.imshow(mnist.train.images[i].reshape(28,28), cmap = 'Greys')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_data = mnist.train.images\n",
    "y_data = mnist.train.labels\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None,784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.int32, [None,10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch는 전체 데이터 중 몇 개의 덩어리로 나눈 개념으로 이해하면 된다.\n",
    "#batch를 쓰는 이유는 전체 데이터의 양이 많기 때문이다-55000개\n",
    "batch_size = 100\n",
    "# print(mnist.train.num_examples)\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    527\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    529\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1272\u001b[0m           \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1273\u001b[1;33m           (dtype.name, value.dtype.name, value))\n\u001b[0m\u001b[0;32m   1274\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor 'Log_2:0' shape=(?, 10) dtype=float32>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-247025588ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1204\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Case: Dense * Sparse.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6699\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6700\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 6701\u001b[1;33m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m   6702\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6703\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    562\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 564\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'."
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "#         if step % 200 == 0:\n",
    "#             loss, acc = sess.run([cost, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "#             print(step, loss, acc)\n",
    "    \n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#     pred = sess.run(prediction, feed_dict={X:x_data})\n",
    "#     for p, y in zip(pred, y_data.flatten()):\n",
    "#         print(\"{} Prediction: {} True Y: {}\".format(p==int(y), p, int(y)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.950021532\n",
      "Epoch: 0002, Cost: 1.107064487\n",
      "Epoch: 0003, Cost: 0.879222766\n",
      "Epoch: 0004, Cost: 0.769486040\n",
      "Epoch: 0005, Cost: 0.701486585\n",
      "Epoch: 0006, Cost: 0.653282802\n",
      "Epoch: 0007, Cost: 0.616952358\n",
      "Epoch: 0008, Cost: 0.587771500\n",
      "Epoch: 0009, Cost: 0.563679084\n",
      "Epoch: 0010, Cost: 0.543365884\n",
      "Epoch: 0011, Cost: 0.525753920\n",
      "Epoch: 0012, Cost: 0.510236868\n",
      "Epoch: 0013, Cost: 0.497128075\n",
      "Epoch: 0014, Cost: 0.485540515\n",
      "Epoch: 0015, Cost: 0.474336218\n",
      "Learning finished\n",
      "Accuracy:  0.8876\n",
      "Label:  [6]\n",
      "Prediction:  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solution-use AdamOptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 6.372004338611263\n",
      "Epoch: 2, Cost: 1.7699949061870561\n",
      "Epoch: 3, Cost: 1.1279595951600514\n",
      "Epoch: 4, Cost: 0.8855546168305652\n",
      "Epoch: 5, Cost: 0.7542261167547917\n",
      "Epoch: 6, Cost: 0.6685921131751748\n",
      "Epoch: 7, Cost: 0.6076707588813524\n",
      "Epoch: 8, Cost: 0.5617945588176899\n",
      "Epoch: 9, Cost: 0.5267963511293586\n",
      "Epoch: 10, Cost: 0.4982657767154955\n",
      "Epoch: 11, Cost: 0.47328929615291765\n",
      "Epoch: 12, Cost: 0.45313409948890876\n",
      "Epoch: 13, Cost: 0.43751067207618194\n",
      "Epoch: 14, Cost: 0.4225788155062635\n",
      "Epoch: 15, Cost: 0.4096402526443652\n",
      "Learning Finish\n",
      "Accuracy: 0.8959000110626221\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15 #epochs DL 용어 - 전체 data를 15번 학습시킴\n",
    "batch_size = 100\n",
    "# batch는 전체 데이터 중 몇 개의 덩어리로 나눈 개념으로 이해하면 된다.\n",
    "#batch를 쓰는 이유는 전체 데이터의 양이 많아 한번에 계산하기 어렵기 때문이다-55000개\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,784]) #image (28x28) ->일렬로 784 한번에 받아들이기\n",
    "Y = tf.placeholder(tf.float32,[None,10]) #Class : 10개 0~9\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,10])) #가중치 784개를 받아서 10개로 mapping\n",
    "b = tf.Variable(tf.random_normal([10])) #10개의 식 각각의 bias\n",
    "\n",
    "hypothesis = tf.matmul(X,W) + b #회귀식 H(x) \n",
    "\n",
    "#softmax , 평균값()\n",
    "#(0,1)의 값 -> cross_entropy사용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "\n",
    "\n",
    "#train \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #epochs만큼 데이터를 학습시킴\n",
    "    #train_size를 batch size로 나눔\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        #batch size만큼 data를 넣어줌\n",
    "        #batch ~ 전체 데이터 크기가 커서 일부에 가중치를 둔다 - batch 개수만큼 가중치를 구함\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict=feed_dict)\n",
    "            #c, _ 가 뭐야? c:cost , _ :optimizer는 안쓰는 data이기 때문에 _를 씀\n",
    "            avg_cost += c / total_batch\n",
    "            #data를 나눠서 가져왔기때문에 cost를 평균값으로 함\n",
    "        #for - batch => 데이터 전체를 한 번 학습시킴\n",
    "    #for - epoch => 데이터 전체를(~fully scan) epoch번 학습시킴. 왜0o0?\n",
    "    #내가 갖고있는 데이터에 fitting을 epoch번 한다. epoch ↑ ~ overfitting(;overfitting=data에만 최적화/ 여기에서의 overfitting은\n",
    "    #최적화된 값을 찾을 때 까지 최대한 학습시키겠다.의 의미로 쓰임(overfitting 될수록 test accuracy 감소)\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch+1, avg_cost))\n",
    "    print(\"Learning Finish\")\n",
    "    \n",
    "    #accuracy - test data\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1)) #axis=1차원\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도 = 숫자값(0,1)~T/F 으로 평균 \n",
    "    print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 160.21847505916244\n",
      "Epoch: 2, Cost: 39.76353066097614\n",
      "Epoch: 3, Cost: 24.89857585083356\n",
      "Epoch: 4, Cost: 17.65349056119267\n",
      "Epoch: 5, Cost: 12.938080259177912\n",
      "Epoch: 6, Cost: 9.692297513457875\n",
      "Epoch: 7, Cost: 7.192963004200586\n",
      "Epoch: 8, Cost: 5.5644629134413846\n",
      "Epoch: 9, Cost: 4.114744757869921\n",
      "Epoch: 10, Cost: 3.1546269818268455\n",
      "Epoch: 11, Cost: 2.4365038536730608\n",
      "Epoch: 12, Cost: 1.8190523774618033\n",
      "Epoch: 13, Cost: 1.35926265719005\n",
      "Epoch: 14, Cost: 1.1066070980785567\n",
      "Epoch: 15, Cost: 0.8882368649253368\n",
      "Learning Finish\n",
      "Accuracy of train data: 0.9907090663909912\n",
      "Accuracy of test data: 0.9447000026702881\n"
     ]
    }
   ],
   "source": [
    "##RELU\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15 #epochs DL 용어\n",
    "batch_size = 100\n",
    "\n",
    "# tf.set_random_seed(100)\n",
    "X = tf.placeholder(tf.float32,[None,784]) #image (28x28) ->일렬로 784 한번에 받아들이기\n",
    "Y = tf.placeholder(tf.float32,[None,10]) #Class : 10개 0~9\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256])) \n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256])) \n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "# hypothesis = tf.nn.relu(tf.matmul(L2,W3) + b3) #회귀식 H(x) \n",
    "#relu~ (0보다 작은값을 지워버림=>음수값은 0으로 fitting)&activate function의 상한선을 없애버림\n",
    "hypothesis = tf.matmul(L2,W3) + b3 #회귀식 H(x) \n",
    "\n",
    "\n",
    "#softmax , 평균값()\n",
    "#(0,1)의 값 -> cross_entropy사용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "\n",
    "\n",
    "#train \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #epochs만큼 데이터를 학습시킴\n",
    "    #train_size를 batch size로 나눔\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        #batch size만큼 data를 넣어줌\n",
    "        #batch ~ 전체 데이터 크기가 커서 일부에 가중치를 둔다 - batch 개수만큼 가중치를 구함\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict=feed_dict)\n",
    "            #c, _ 가 뭐야? c:cost , _ :optimizer는 안쓰는 data이기 때문에 _를 씀\n",
    "            avg_cost += c / total_batch\n",
    "            #data를 나눠서 가져왔기때문에 cost를 평균값으로 함\n",
    "        #for - batch => 데이터 전체를 한 번 학습시킴\n",
    "    #for - epoch => 데이터 전체를(~fully scan) epoch번 학습시킴. 왜0o0?\n",
    "    #내가 갖고있는 데이터에 fitting을 epoch번 한다. epoch ↑ ~ overfitting(;overfitting=data에만 최적화/ 여기에서의 overfitting은\n",
    "    #최적화된 값을 찾을 때 까지 최대한 학습시키겠다.의 의미로 쓰임(overfitting 될수록 test accuracy 감소)\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch+1, avg_cost))\n",
    "    print(\"Learning Finish\")\n",
    "    \n",
    "    #accuracy - test data\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1)) #axis=1차원\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도 = 숫자값(0,1)~T/F 으로 평균 \n",
    "    print(\"Accuracy of train data: {}\".format(sess.run(accuracy, feed_dict={X:mnist.train.images, Y:mnist.train.labels})))\n",
    "    print(\"Accuracy of test data: {}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
       "       0.5921569 , 0.23529413, 0.14117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.8705883 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9450981 , 0.77647066, 0.77647066, 0.77647066, 0.77647066,\n",
       "       0.77647066, 0.77647066, 0.77647066, 0.77647066, 0.6666667 ,\n",
       "       0.20392159, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2627451 , 0.44705886,\n",
       "       0.28235295, 0.44705886, 0.6392157 , 0.89019614, 0.9960785 ,\n",
       "       0.882353  , 0.9960785 , 0.9960785 , 0.9960785 , 0.9803922 ,\n",
       "       0.8980393 , 0.9960785 , 0.9960785 , 0.54901963, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "       0.2627451 , 0.2627451 , 0.23137257, 0.08235294, 0.92549026,\n",
       "       0.9960785 , 0.4156863 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.3254902 , 0.9921569 , 0.8196079 , 0.07058824,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.91372555,\n",
       "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5058824 , 0.9960785 , 0.9333334 , 0.17254902,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137257, 0.97647065,\n",
       "       0.9960785 , 0.24313727, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.52156866, 0.9960785 , 0.73333335, 0.01960784,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03529412, 0.80392164,\n",
       "       0.9725491 , 0.227451  , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.49411768, 0.9960785 , 0.7137255 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 0.9843138 ,\n",
       "       0.94117653, 0.22352943, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.07450981, 0.86666673, 0.9960785 , 0.6509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.7960785 , 0.9960785 ,\n",
       "       0.8588236 , 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.9960785 , 0.9960785 , 0.3019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156864, 0.87843144, 0.9960785 ,\n",
       "       0.45098042, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.9960785 , 0.9960785 , 0.20392159, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.2392157 , 0.9490197 , 0.9960785 ,\n",
       "       0.9960785 , 0.20392159, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.47450984, 0.9960785 , 0.9960785 , 0.8588236 , 0.15686275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.47450984, 0.9960785 ,\n",
       "       0.8117648 , 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images #2차원 array\n",
    "mnist.test.images.shape\n",
    "mnist.test.images[0] #0~1사이 값을 갖는 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mywritten_nine.png\n",
    "##내가 쓴 글씨를 테스트해보자!\n",
    "#image를 import하는 package\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "im = Image.open(\"mywritten_nine.png\")\n",
    "pixel = np.array(im)\n",
    "pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3 = Image.open(\"mywritten_three.png\")\n",
    "pixel1 = np.array(im3)\n",
    "pixel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytestimg = 1 - pixel[:,:,0]/255 \n",
    "#검정색:1 , 흰색:0\n",
    "mytestimg = mytestimg.reshape(1,784) #1개의 data가 784로 갖고있다. 여러개의 data=>none,784\n",
    "mytestimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytestimg1 = 1 - pixel1[:,:,0]/255 \n",
    "#검정색:1 , 흰색:0\n",
    "mytestimg1 = mytestimg1.reshape(1,784) #1개의 data가 784로 갖고있다. 여러개의 data=>none,784\n",
    "mytestimg1.shape\n",
    "# mytest=[mytestimg,mytestimg1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 168.853983523629\n",
      "Epoch: 2, Cost: 41.370346309488525\n",
      "Epoch: 3, Cost: 25.15873206875541\n",
      "Epoch: 4, Cost: 17.950760260495283\n",
      "Epoch: 5, Cost: 12.279062504226514\n",
      "Epoch: 6, Cost: 9.767962709407925\n",
      "Epoch: 7, Cost: 6.971358753590198\n",
      "Epoch: 8, Cost: 5.431878322108264\n",
      "Epoch: 9, Cost: 4.081774345923506\n",
      "Epoch: 10, Cost: 3.0603869925576506\n",
      "Epoch: 11, Cost: 2.3057003147041764\n",
      "Epoch: 12, Cost: 1.8279515336175562\n",
      "Epoch: 13, Cost: 1.3086358341714848\n",
      "Epoch: 14, Cost: 1.0216321512167188\n",
      "Epoch: 15, Cost: 0.8356333201842875\n",
      "Epoch: 16, Cost: 0.728710834102938\n",
      "Epoch: 17, Cost: 0.688880172607471\n",
      "Epoch: 18, Cost: 0.5042375190398534\n",
      "Epoch: 19, Cost: 0.4979157874714035\n",
      "Epoch: 20, Cost: 0.3800074108346984\n",
      "Learning Finish\n",
      "Accuracy of train data: 0.9953454732894897\n",
      "Accuracy of test data: 0.9527999758720398\n",
      "Prediction of 9: [[-552.9031    590.6263   1043.2035   1158.5989   1383.2513   -409.04852\n",
      "   567.03204  1269.8812    122.642586  666.1975  ]]\n",
      "Prediction of 3: [[-283.3379  1165.9186  2341.6687  2265.061   -520.74023 1353.6487\n",
      "   927.2744   613.1248   881.6318   115.4762 ]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 20 #epochs DL 용어\n",
    "batch_size = 100\n",
    "\n",
    "# tf.set_random_seed(100)\n",
    "X = tf.placeholder(tf.float32,[None,784]) #image (28x28) ->일렬로 784 한번에 받아들이기\n",
    "Y = tf.placeholder(tf.float32,[None,10]) #Class : 10개 0~9\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256])) \n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256])) \n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "# hypothesis = tf.nn.relu(tf.matmul(L2,W3) + b3) #회귀식 H(x) \n",
    "#relu~ (0보다 작은값을 지워버림=>음수값은 0으로 fitting)&activate function의 상한선을 없애버림\n",
    "hypothesis = tf.matmul(L2,W3) + b3 #회귀식 H(x) \n",
    "\n",
    "\n",
    "#softmax , 평균값()\n",
    "#(0,1)의 값 -> cross_entropy사용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "\n",
    "\n",
    "#train \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #epochs만큼 데이터를 학습시킴\n",
    "    #train_size를 batch size로 나눔\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        #batch size만큼 data를 넣어줌\n",
    "        #batch ~ 전체 데이터 크기가 커서 일부에 가중치를 둔다 - batch 개수만큼 가중치를 구함\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict=feed_dict)\n",
    "            #c, _ 가 뭐야? c:cost , _ :optimizer는 안쓰는 data이기 때문에 _를 씀\n",
    "            avg_cost += c / total_batch\n",
    "            #data를 나눠서 가져왔기때문에 cost를 평균값으로 함\n",
    "        #for - batch => 데이터 전체를 한 번 학습시킴\n",
    "    #for - epoch => 데이터 전체를(~fully scan) epoch번 학습시킴. 왜0o0?\n",
    "    #내가 갖고있는 데이터에 fitting을 epoch번 한다. epoch ↑ ~ overfitting(;overfitting=data에만 최적화/ 여기에서의 overfitting은\n",
    "    #최적화된 값을 찾을 때 까지 최대한 학습시키겠다.의 의미로 쓰임(overfitting 될수록 test accuracy 감소)\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch+1, avg_cost))\n",
    "    print(\"Learning Finish\")\n",
    "    \n",
    "    #accuracy - test data\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1)) #axis=1차원\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도 = 숫자값(0,1)~T/F 으로 평균 \n",
    "    print(\"Accuracy of train data: {}\".format(sess.run(accuracy, feed_dict={X:mnist.train.images, Y:mnist.train.labels})))\n",
    "    print(\"Accuracy of test data: {}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})))\n",
    "    print(\"Prediction of 9: {}\".format(sess.run(hypothesis, feed_dict={X:mytestimg}))) # Y:[[0,0,0,0,0,0,0,0,0,1]]})))\n",
    "    print(\"Prediction of 3: {}\".format(sess.run(hypothesis, feed_dict={X:mytestimg1})))#  Y:[[0,0,0,1,0,0,0,0,0,0]]})))\n",
    "#     pred = sess.run(hypothesis,feed_dict={X:mytestimg})\n",
    "#     print(\"Prediction of my data: {}\".format(np.max(pred==np.max(pred))))\n",
    "#     Y:[[0,0,0,0,0,0,0,0,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
