{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0525758\n",
      "200 0.21075541\n",
      "400 0.17872053\n",
      "600 0.15975931\n",
      "800 0.14659703\n",
      "1000 0.13650969\n",
      "1200 0.12827197\n",
      "1400 0.121261746\n",
      "1600 0.11513205\n",
      "1800 0.10967347\n",
      "2000 0.104750216\n",
      "[[3.1987613e-01 6.8002141e-01 1.0248713e-04]] [1]\n"
     ]
    }
   ],
   "source": [
    "##여러개 클래스를 가지는 logistic regression\n",
    "\n",
    "x_data = [[1,2,1,1],\n",
    "          [2,1,3,2],\n",
    "          [3,1,3,4],\n",
    "          [4,1,5,5],\n",
    "          [1,7,5,5],\n",
    "          [1,2,5,6],\n",
    "          [1,6,6,6],\n",
    "          [1,7,7,7]]\n",
    "y_data = [[0,0,1],\n",
    "          [0,0,1],\n",
    "          [0,0,1],\n",
    "          [0,1,0],\n",
    "          [0,1,0],\n",
    "          [0,1,0],\n",
    "          [1,0,0],\n",
    "          [1,0,0]]\n",
    "#3개의 클래스\n",
    "\n",
    "X = tf.placeholder(\"float\", [None,4]) # [None,4]=?행 4열 x_data와 같은 열개수를 가짐 \n",
    "Y = tf.placeholder(\"float\", [None,3]) # ?행 3열 work as placeholder\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4,3]), name='weight') #4x3 행렬\n",
    "b = tf.Variable(tf.random_normal([3]), name='bias') #3x1 행렬; broadcasting\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)  #softmax적용  X*W = (8x4) * (4x3) = (8x3)\n",
    "\n",
    "#설명\n",
    "#softmax는 두 가지 역할을 수행한다.\n",
    "#1. 입력을 sigmoid와 마찬가지로 0과 1 사이의 값으로 변환한다.\n",
    "#2. 변환된 결과에 대한 합계가 1이 되도록 만들어 준다.\n",
    "\n",
    "#cost 함수는 예측한 값과 실제 값의 거리(distance, D)를 계산하는 함수로, 이 값이 줄어드는 방향으로, \n",
    "#즉 entropy가 감소하는 방향으로 진행하다 보면 최저점을 만나게 된다.\n",
    "\n",
    "#cost = error 값 계산\n",
    "cost = tf.reduce_mean(-tf.reduce_mean(Y * tf.log(hypothesis), axis=1)) \n",
    "#reduce_mean(entropy 함수)\n",
    "#entropy 함수; -tf.reduce_mean(Y * tf.log(hypothesis), axis = 1)\n",
    "# hypothesis의 가장 큰 확률과 기대클래스와의 거리를 측정\n",
    "# 여러개의 예제의 기대값과의 거리의 평균을 구함 \n",
    "\n",
    "#경사하강법, learning_rate=0.1, cost의 최소값을 찾음\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 200 == 0: #200번 마다 출력 (step, cost, feed_dict)\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    a = sess.run(hypothesis, feed_dict={X:[[1,11,7,9]]})\n",
    "    #feed_dict = [1,11,7,9] ~1개 판단 \n",
    "    print(a, sess.run(tf.argmax(a,1)))\n",
    "    #argmax : 최댓값의 index반환\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16)\n"
     ]
    }
   ],
   "source": [
    "##SOFTMAX분류\n",
    "#data-04-zoo.csv불러옴\n",
    "import numpy as np\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:,0:-1] #마지막 열을 제외한 데이터 추출\n",
    "y_data = xy[:,[-1]] # 마지막 열만 추출 (2차원 형식)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot_12:0\", shape=(?, 1, 7), dtype=float32)\n",
      "Tensor(\"Reshape_12:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None,16])\n",
    "Y = tf.placeholder(tf.int32, [None,1])\n",
    "\n",
    "# one-hot encoding은 softmax로 구한 값 중에서 가장 큰 값을 1로, 나머지를 0으로 만든다.\n",
    "Y_one_hot = tf.one_hot(Y, 7) #index크기로 분류\n",
    "print(Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1,7])\n",
    "print(Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: Tensor(\"add_48:0\", shape=(?, 7), dtype=float32)\n",
      "0 2.5469327 0.28712872\n",
      "200 0.38453516 0.8811881\n",
      "400 0.22326255 0.9405941\n",
      "600 0.15555367 0.980198\n",
      "800 0.11909114 1.0\n",
      "1000 0.096830346 1.0\n",
      "1200 0.08191575 1.0\n",
      "1400 0.07119573 1.0\n",
      "1600 0.06308465 1.0\n",
      "1800 0.056711145 1.0\n",
      "2000 0.05155768 1.0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 5 True Y: 5\n",
      "True Prediction: 4 True Y: 4\n",
      "True Prediction: 4 True Y: 4\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 5 True Y: 5\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 5 True Y: 5\n",
      "True Prediction: 5 True Y: 5\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 5 True Y: 5\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 5 True Y: 5\n",
      "True Prediction: 4 True Y: 4\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 2 True Y: 2\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 2 True Y: 2\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 2 True Y: 2\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 5 True Y: 5\n",
      "True Prediction: 4 True Y: 4\n",
      "True Prediction: 2 True Y: 2\n",
      "True Prediction: 2 True Y: 2\n",
      "True Prediction: 3 True Y: 3\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 1 True Y: 1\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 5 True Y: 5\n",
      "True Prediction: 0 True Y: 0\n",
      "True Prediction: 6 True Y: 6\n",
      "True Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_normal([16,7]), name='weight') #(16x7)\n",
    "b = tf.Variable(tf.random_normal([7]), name='bias') #(7x1)\n",
    "logits = tf.matmul(X,W) + b  #y= XW + b\n",
    "hypothesis = tf.nn.softmax(logits) #softmax\n",
    "\n",
    "#softmax_cross_entropy_with_logits - logits 을 이용한 softmax\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost) #경사하강법-비용 최솟값 찾기\n",
    "\n",
    "#argmax : 최댓값의 index반환\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#accuracy(정확도) = 전체에서 prediction과 correct_prediction이 같은 확률  \n",
    "\n",
    "#그래프 출력\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"logits:\",logits)\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X:x_data, Y:y_data}) #placeholder에 넣을 값\n",
    "        if step % 200 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "            print(step, loss, acc)\n",
    "    pred = sess.run(prediction, feed_dict={X:x_data})\n",
    "    for p, y in zip(pred, y_data.flatten()): #flatten() : 다차원 배열을 1차원 배열로 바꿔줌\n",
    "        print(\"{} Prediction: {} True Y: {}\".format(p==int(y), p, int(y)))\n",
    "        #T/F:prediction값과 true y 값이 같은지,Prediction,True Y value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7898594\n",
      "200 0.72449505\n",
      "400 0.707556\n",
      "600 0.7023467\n",
      "800 0.69998735\n",
      "1000 0.69848996\n",
      "1200 0.697381\n",
      "1400 0.69651806\n",
      "1600 0.6958372\n",
      "1800 0.6952974\n",
      "2000 0.69486874\n",
      "2200 0.694528\n",
      "2400 0.69425666\n",
      "2600 0.69404024\n",
      "2800 0.69386744\n",
      "3000 0.6937293\n",
      "3200 0.6936186\n",
      "3400 0.6935297\n",
      "3600 0.69345826\n",
      "3800 0.69340074\n",
      "4000 0.69335425\n",
      "4200 0.69331664\n",
      "4400 0.69328624\n",
      "4600 0.6932615\n",
      "4800 0.6932413\n",
      "5000 0.6932249\n",
      "5200 0.6932115\n",
      "5400 0.69320047\n",
      "5600 0.6931914\n",
      "5800 0.69318396\n",
      "6000 0.6931778\n",
      "6200 0.6931728\n",
      "6400 0.6931686\n",
      "6600 0.6931651\n",
      "6800 0.6931622\n",
      "7000 0.6931598\n",
      "7200 0.6931578\n",
      "7400 0.6931561\n",
      "7600 0.6931547\n",
      "7800 0.6931535\n",
      "8000 0.69315255\n",
      "8200 0.6931517\n",
      "8400 0.693151\n",
      "8600 0.6931504\n",
      "8800 0.6931499\n",
      "9000 0.6931495\n",
      "9200 0.6931491\n",
      "9400 0.69314885\n",
      "9600 0.6931486\n",
      "9800 0.6931484\n",
      "10000 0.69314826\n",
      "\n",
      "Hypothesis:  [[0.49886686]\n",
      " [0.49958882]\n",
      " [0.50005555]\n",
      " [0.5007775 ]] \n",
      "Predicted:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2]) #(4x2)\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1]) #(4x1)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight') # 가중치 (2x1)\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias') #bias (1) broadcasting\n",
    "\n",
    "#가설H(x)(예측값)을 sigmoid 함수로 정의 ; XW + b\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b) #1.0 / (1 + tf.exp(-(tf.matmul(X,W) + b)))\n",
    "\n",
    "#cost function 새롭게 정의\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "#경사하강법\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "#가설 H(x) > 0.5 = T(1)/ else: F(0)\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "#accuracy(정확도) 0이나 1의 값을 트레이닝 횟수만큼 평균치 계산\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "#cost 값이 최소화 되는지 관찰\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)   \n",
    "            #print(step, cost)\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], \n",
    "                       feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h,\n",
    "          \"\\nPredicted: \", c,\n",
    "          \"\\nAccuracy: \", a)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.69121695\n",
      "2000 0.68082845\n",
      "4000 0.6707556\n",
      "6000 0.65669674\n",
      "8000 0.6383492\n",
      "10000 0.6171942\n",
      "\n",
      "Hypothesis:  [[0.46748286]\n",
      " [0.72110605]\n",
      " [0.3870416 ]\n",
      " [0.43016076]] \n",
      "Predicted:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "##XOR 분류 Neural Network\n",
    "\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2]) #4x2\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1]) #4x1\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2,2]), name='weight')  #가중치\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias')  #bias\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1) #hidden Layer 1\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2) + b2) #Layer 2, output\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, cost_val)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], \n",
    "                       feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h,\n",
    "          \"\\nPredicted: \", c,\n",
    "          \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7085786\n",
      "2000 0.6872022\n",
      "4000 0.6743882\n",
      "6000 0.6582514\n",
      "8000 0.631835\n",
      "10000 0.5828123\n",
      "\n",
      "Hypothesis:  [[0.45576856]\n",
      " [0.58517087]\n",
      " [0.51660156]\n",
      " [0.40927187]] \n",
      "Predicted:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2,10]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,10]), name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10,1]), name='weight')\n",
    "b3 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, cost_val)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], \n",
    "                       feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h,\n",
    "          \"\\nPredicted: \", c,\n",
    "          \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
