{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b37328c5bd0e>, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-b37328c5bd0e>\"\u001b[1;36m, line \u001b[1;32m77\u001b[0m\n\u001b[1;33m    0 0.3000054\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#예시\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./concrete.csv\", sep=\",\", header=0)\n",
    "data\n",
    "\n",
    "def feture_scaling(df, scaling_strategy=\"min-max\", column=None):\n",
    "    if column == None:\n",
    "        column = [column_name for column_name in df.columns]\n",
    "    for column_name in column:\n",
    "        if scaling_strategy == \"min-max\":\n",
    "            df[column_name] = ( df[column_name] - df[column_name].min() ) /\\\n",
    "                            (df[column_name].max() - df[column_name].min()) \n",
    "        elif scaling_strategy == \"z-score\":\n",
    "            df[column_name] = ( df[column_name] - \\\n",
    "                               df[column_name].mean() ) /\\\n",
    "                            (df[column_name].std() )\n",
    "    return df\n",
    "\n",
    "feture_scaling(data,column=[\"cement\",\"slag\",\"ash\",\"water\",\"superplastic\",\"coarseagg\",\"fineagg\",\"age\",\"strength\"])\n",
    "data.head\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "x_data = data[[\"cement\",\"slag\",\"ash\",\"water\",\"superplastic\",\"coarseagg\",\"fineagg\",\"age\"]]\n",
    "print(x_data.shape)\n",
    "y_data = data[[\"strength\"]]\n",
    "print(y_data.shape)\n",
    "\n",
    "num_node = 10\n",
    "num_layer = 2\n",
    "learn_rate = 0.01\n",
    "\n",
    "W_ = []\n",
    "b_ = []\n",
    "layer_ = []\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([8,num_node]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([num_node]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W_.append(W1)\n",
    "b_.append(b1)\n",
    "layer_.append(layer1)\n",
    "\n",
    "for i in range(1, num_layer):\n",
    "    W_.append(tf.Variable(tf.random_normal([num_node,num_node]), name='weight'))\n",
    "    b_.append(tf.Variable(tf.random_normal([num_node]), name='bias'))\n",
    "    layer_.append(tf.sigmoid(tf.matmul(layer_[i-1], W_[i]) + b_[i]))\n",
    "\n",
    "W_last = tf.Variable(tf.random_normal([num_node,1]), name='weight')\n",
    "b_last = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "Y_hat = tf.sigmoid(tf.matmul(layer_[-1], W_last) + b_last)\n",
    "loss = tf.reduce_mean(tf.square(Y - Y_hat))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learn_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_history = []\n",
    "    for step in range(10001):\n",
    "        cost_val, hypothesis, _ = sess.run([loss, Y_hat, train], feed_dict={X:x_data, Y:y_data}) # ì—°ì‚°ê·¸ëž˜í”„ ì‹¤í–‰í•  ë•Œ ë³€ìˆ˜ ê°’ì„ ë„£ìŒ\n",
    "        cost_history.append(cost_val)\n",
    "        if step % 2000 == 0:\n",
    "            print(step, cost_val)\n",
    "    plt.plot(cost_history)\n",
    "    plt.show()\n",
    "\n",
    "    pred = sess.run(Y_hat, feed_dict={X:x_data})\n",
    "    print(np.cov(np.array(y_data).reshape(-1), np.array(pred).reshape))\n",
    "#result\n",
    "# (1030, 8)\n",
    "# (1030, 1)\n",
    "# 0 0.3000054\n",
    "# 2000 0.04737836\n",
    "# 4000 0.04028104\n",
    "# 6000 0.03553781\n",
    "# 8000 0.03276191\n",
    "# 10000 0.030917015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분산이 작을수록 평균값(예측값)과 가까움  \n",
    "공분산 covariance  \n",
    "공분산 값이 커지면 분산도 커지고... cov  \n",
    "공분산이 작을수록 예측값과 분산이 가까워짐  \n",
    "minmax normalization (0,1)사이 구간 (마이너스가 의미가 없음/약해짐->(-)또한 0~1사이 구간으로 옮겨지기 때문)  \n",
    "z- score standardization 마이너스가 의미가 있는 데이터 (입/출, 방향, 재무구조 등)  \n",
    "\n",
    "ML은 정규화가 필수  \n",
    "z-score standardization  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.25882354 0.7607844\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  1.         0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.6627451  0.14509805 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.882353   0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.86666673 0.08235294 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.882353   0.9921569  0.9921569  0.9607844  0.8313726  0.9294118\n",
      " 0.9607844  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.45098042 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.22352943 0.25490198\n",
      " 0.25490198 0.21176472 0.05882353 0.1764706  0.21176472 0.25490198\n",
      " 0.41960788 0.9921569  0.9921569  0.9921569  0.90196085 0.21568629\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6745098  0.9921569\n",
      " 0.9921569  0.9921569  0.6666667  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.15686275 0.83921576 0.9921569  0.9921569  0.9921569  0.6901961\n",
      " 0.17254902 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01568628 0.13333334 0.627451   0.9686275  0.9921569\n",
      " 0.9921569  0.9921569  0.882353   0.14117648 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.03921569 0.09411766 0.35686275 0.8313726\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.8862746\n",
      " 0.03529412 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.04705883 0.32941177\n",
      " 0.7803922  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.75294125 0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.28235295 0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9176471  0.25490198 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.19607845 0.93725497 0.9921569  0.8431373  0.6901961  0.6901961\n",
      " 0.58431375 0.37254903 0.3019608  0.6509804  0.8470589  0.9921569\n",
      " 0.9921569  0.9921569  0.6666667  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.12941177\n",
      " 0.16078432 0.08235294 0.         0.         0.         0.\n",
      " 0.         0.         0.10588236 0.83921576 0.9921569  0.9921569\n",
      " 0.6666667  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1254902  0.08235294 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.5803922  0.9921569  0.9921569  0.6666667  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00784314 0.6745098\n",
      " 0.9803922  0.6156863  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.26666668 0.79215693\n",
      " 0.9921569  0.9921569  0.6666667  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.14117648 0.9921569  0.9607844  0.16078432\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.7843138  0.9921569  0.9921569  0.9921569\n",
      " 0.6117647  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.44705886 0.9921569  0.8980393  0.12156864 0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.7607844\n",
      " 0.9921569  0.9921569  0.9921569  0.9450981  0.16862746 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.27058825 0.9921569\n",
      " 0.9921569  0.60784316 0.03137255 0.         0.         0.\n",
      " 0.18039216 0.40000004 0.8196079  0.9921569  0.9921569  0.9921569\n",
      " 0.9450981  0.33333334 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09411766 0.89019614 0.9921569  0.9921569\n",
      " 0.8117648  0.78823537 0.78823537 0.78823537 0.9333334  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9568628  0.32941177 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.19215688 0.70980394 0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.6039216  0.3137255  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.23137257 0.94117653 0.9921569  0.9921569  0.9921569\n",
      " 0.9921569  0.85098046 0.35686275 0.06666667 0.00392157 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "(array([3], dtype=int64),)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "(array([8], dtype=int64),)\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "(array([6], dtype=int64),)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "(array([5], dtype=int64),)\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "(array([6], dtype=int64),)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#images <실제적 데이터, y<label\n",
    "#data 28x28 \n",
    "#accuracy ~ test data : mnist.test.images[i]\n",
    "#softmax& NN\n",
    "print(mnist.train.images[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print(mnist.train.labels[i])\n",
    "    print(np.where(mnist.train.labels[i] == 1))\n",
    "    plt.imshow(mnist.train.images[i].reshape(28,28), cmap = 'Greys')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_data = mnist.train.images\n",
    "y_data = mnist.train.labels\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None,784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.int32, [None,10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch는 전체 데이터 중 몇 개의 덩어리로 나눈 개념으로 이해하면 된다.\n",
    "#batch를 쓰는 이유는 전체 데이터의 양이 많기 때문이다-55000개\n",
    "batch_size = 100\n",
    "# print(mnist.train.num_examples)\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    527\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    529\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1272\u001b[0m           \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1273\u001b[1;33m           (dtype.name, value.dtype.name, value))\n\u001b[0m\u001b[0;32m   1274\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor 'Log_2:0' shape=(?, 10) dtype=float32>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-247025588ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1204\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Case: Dense * Sparse.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6699\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6700\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 6701\u001b[1;33m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m   6702\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6703\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    562\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 564\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'."
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "#         if step % 200 == 0:\n",
    "#             loss, acc = sess.run([cost, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "#             print(step, loss, acc)\n",
    "    \n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#     pred = sess.run(prediction, feed_dict={X:x_data})\n",
    "#     for p, y in zip(pred, y_data.flatten()):\n",
    "#         print(\"{} Prediction: {} True Y: {}\".format(p==int(y), p, int(y)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.950021532\n",
      "Epoch: 0002, Cost: 1.107064487\n",
      "Epoch: 0003, Cost: 0.879222766\n",
      "Epoch: 0004, Cost: 0.769486040\n",
      "Epoch: 0005, Cost: 0.701486585\n",
      "Epoch: 0006, Cost: 0.653282802\n",
      "Epoch: 0007, Cost: 0.616952358\n",
      "Epoch: 0008, Cost: 0.587771500\n",
      "Epoch: 0009, Cost: 0.563679084\n",
      "Epoch: 0010, Cost: 0.543365884\n",
      "Epoch: 0011, Cost: 0.525753920\n",
      "Epoch: 0012, Cost: 0.510236868\n",
      "Epoch: 0013, Cost: 0.497128075\n",
      "Epoch: 0014, Cost: 0.485540515\n",
      "Epoch: 0015, Cost: 0.474336218\n",
      "Learning finished\n",
      "Accuracy:  0.8876\n",
      "Label:  [6]\n",
      "Prediction:  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solution-use AdamOptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 6.372004338611263\n",
      "Epoch: 2, Cost: 1.7699949061870561\n",
      "Epoch: 3, Cost: 1.1279595951600514\n",
      "Epoch: 4, Cost: 0.8855546168305652\n",
      "Epoch: 5, Cost: 0.7542261167547917\n",
      "Epoch: 6, Cost: 0.6685921131751748\n",
      "Epoch: 7, Cost: 0.6076707588813524\n",
      "Epoch: 8, Cost: 0.5617945588176899\n",
      "Epoch: 9, Cost: 0.5267963511293586\n",
      "Epoch: 10, Cost: 0.4982657767154955\n",
      "Epoch: 11, Cost: 0.47328929615291765\n",
      "Epoch: 12, Cost: 0.45313409948890876\n",
      "Epoch: 13, Cost: 0.43751067207618194\n",
      "Epoch: 14, Cost: 0.4225788155062635\n",
      "Epoch: 15, Cost: 0.4096402526443652\n",
      "Learning Finish\n",
      "Accuracy: 0.8959000110626221\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15 #epochs DL 용어\n",
    "batch_size = 100\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,784]) #image (28x28) ->일렬로 784 한번에 받아들이기\n",
    "Y = tf.placeholder(tf.float32,[None,10]) #Class : 10개 0~9\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,10])) #가중치 784개를 받아서 10개로 mapping\n",
    "b = tf.Variable(tf.random_normal([10])) #10개의 식 각각의 bias\n",
    "\n",
    "hypothesis = tf.matmul(X,W) + b #회귀식 H(x) \n",
    "\n",
    "#softmax , 평균값()\n",
    "#(0,1)의 값 -> cross_entropy사용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "\n",
    "\n",
    "#train \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #epochs만큼 데이터를 학습시킴\n",
    "    #train_size를 batch size로 나눔\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        #batch size만큼 data를 넣어줌\n",
    "        #batch ~ 전체 데이터 크기가 커서 일부에 가중치를 둔다 - batch 개수만큼 가중치를 구함\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict=feed_dict)\n",
    "            #c, _ 가 뭐야? c:cost , _ :optimizer는 안쓰는 data이기 때문에 _를 씀\n",
    "            avg_cost += c / total_batch\n",
    "            #data를 나눠서 가져왔기때문에 cost를 평균값으로 함\n",
    "        #for - batch => 데이터 전체를 한 번 학습시킴\n",
    "    #for - epoch => 데이터 전체를(~fully scan) epoch번 학습시킴. 왜0o0?\n",
    "    #내가 갖고있는 데이터에 fitting을 epoch번 한다. epoch ↑ ~ overfitting(;overfitting=data에만 최적화/ 여기에서의 overfitting은\n",
    "    #최적화된 값을 찾을 때 까지 최대한 학습시키겠다.의 의미로 쓰임(overfitting 될수록 test accuracy 감소)\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch+1, avg_cost))\n",
    "    print(\"Learning Finish\")\n",
    "    \n",
    "    #accuracy - test data\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1)) #axis=1차원\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도 = 숫자값(0,1)~T/F 으로 평균 \n",
    "    print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 160.21847505916244\n",
      "Epoch: 2, Cost: 39.76353066097614\n",
      "Epoch: 3, Cost: 24.89857585083356\n",
      "Epoch: 4, Cost: 17.65349056119267\n",
      "Epoch: 5, Cost: 12.938080259177912\n",
      "Epoch: 6, Cost: 9.692297513457875\n",
      "Epoch: 7, Cost: 7.192963004200586\n",
      "Epoch: 8, Cost: 5.5644629134413846\n",
      "Epoch: 9, Cost: 4.114744757869921\n",
      "Epoch: 10, Cost: 3.1546269818268455\n",
      "Epoch: 11, Cost: 2.4365038536730608\n",
      "Epoch: 12, Cost: 1.8190523774618033\n",
      "Epoch: 13, Cost: 1.35926265719005\n",
      "Epoch: 14, Cost: 1.1066070980785567\n",
      "Epoch: 15, Cost: 0.8882368649253368\n",
      "Learning Finish\n",
      "Accuracy of train data: 0.9907090663909912\n",
      "Accuracy of test data: 0.9447000026702881\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15 #epochs DL 용어\n",
    "batch_size = 100\n",
    "\n",
    "# tf.set_random_seed(100)\n",
    "X = tf.placeholder(tf.float32,[None,784]) #image (28x28) ->일렬로 784 한번에 받아들이기\n",
    "Y = tf.placeholder(tf.float32,[None,10]) #Class : 10개 0~9\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256])) \n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256])) \n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "# hypothesis = tf.nn.relu(tf.matmul(L2,W3) + b3) #회귀식 H(x) \n",
    "#relu~ (0보다 작은값을 지워버림=>음수값은 0으로 fitting)&activate function의 상한선을 없애버림\n",
    "hypothesis = tf.matmul(L2,W3) + b3 #회귀식 H(x) \n",
    "\n",
    "\n",
    "#softmax , 평균값()\n",
    "#(0,1)의 값 -> cross_entropy사용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "\n",
    "\n",
    "#train \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #epochs만큼 데이터를 학습시킴\n",
    "    #train_size를 batch size로 나눔\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        #batch size만큼 data를 넣어줌\n",
    "        #batch ~ 전체 데이터 크기가 커서 일부에 가중치를 둔다 - batch 개수만큼 가중치를 구함\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict=feed_dict)\n",
    "            #c, _ 가 뭐야? c:cost , _ :optimizer는 안쓰는 data이기 때문에 _를 씀\n",
    "            avg_cost += c / total_batch\n",
    "            #data를 나눠서 가져왔기때문에 cost를 평균값으로 함\n",
    "        #for - batch => 데이터 전체를 한 번 학습시킴\n",
    "    #for - epoch => 데이터 전체를(~fully scan) epoch번 학습시킴. 왜0o0?\n",
    "    #내가 갖고있는 데이터에 fitting을 epoch번 한다. epoch ↑ ~ overfitting(;overfitting=data에만 최적화/ 여기에서의 overfitting은\n",
    "    #최적화된 값을 찾을 때 까지 최대한 학습시키겠다.의 의미로 쓰임(overfitting 될수록 test accuracy 감소)\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch+1, avg_cost))\n",
    "    print(\"Learning Finish\")\n",
    "    \n",
    "    #accuracy - test data\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1)) #axis=1차원\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도 = 숫자값(0,1)~T/F 으로 평균 \n",
    "    print(\"Accuracy of train data: {}\".format(sess.run(accuracy, feed_dict={X:mnist.train.images, Y:mnist.train.labels})))\n",
    "    print(\"Accuracy of test data: {}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
       "       0.5921569 , 0.23529413, 0.14117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.8705883 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9450981 , 0.77647066, 0.77647066, 0.77647066, 0.77647066,\n",
       "       0.77647066, 0.77647066, 0.77647066, 0.77647066, 0.6666667 ,\n",
       "       0.20392159, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2627451 , 0.44705886,\n",
       "       0.28235295, 0.44705886, 0.6392157 , 0.89019614, 0.9960785 ,\n",
       "       0.882353  , 0.9960785 , 0.9960785 , 0.9960785 , 0.9803922 ,\n",
       "       0.8980393 , 0.9960785 , 0.9960785 , 0.54901963, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "       0.2627451 , 0.2627451 , 0.23137257, 0.08235294, 0.92549026,\n",
       "       0.9960785 , 0.4156863 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.3254902 , 0.9921569 , 0.8196079 , 0.07058824,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.91372555,\n",
       "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5058824 , 0.9960785 , 0.9333334 , 0.17254902,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137257, 0.97647065,\n",
       "       0.9960785 , 0.24313727, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.52156866, 0.9960785 , 0.73333335, 0.01960784,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03529412, 0.80392164,\n",
       "       0.9725491 , 0.227451  , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.49411768, 0.9960785 , 0.7137255 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 0.9843138 ,\n",
       "       0.94117653, 0.22352943, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.07450981, 0.86666673, 0.9960785 , 0.6509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.7960785 , 0.9960785 ,\n",
       "       0.8588236 , 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.9960785 , 0.9960785 , 0.3019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156864, 0.87843144, 0.9960785 ,\n",
       "       0.45098042, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.9960785 , 0.9960785 , 0.20392159, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.2392157 , 0.9490197 , 0.9960785 ,\n",
       "       0.9960785 , 0.20392159, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.47450984, 0.9960785 , 0.9960785 , 0.8588236 , 0.15686275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.47450984, 0.9960785 ,\n",
       "       0.8117648 , 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images #2차원 array\n",
    "mnist.test.images.shape\n",
    "mnist.test.images[0] #0~1사이 값을 갖는 data를 찾아내주면 됨. matrix형태로 바꾸면되는데..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mywritten_nine.png\n",
    "##내가 쓴 글씨를 테스트해보자!\n",
    "#image를 import하는 package\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "im = Image.open(\"mywritten_nine.png\")\n",
    "pixel = np.array(im)\n",
    "pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3 = Image.open(\"mywritten_three.png\")\n",
    "pixel1 = np.array(im3)\n",
    "pixel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytestimg = 1 - pixel[:,:,0]/255 \n",
    "#검정색:1 , 흰색:0\n",
    "mytestimg = mytestimg.reshape(1,784) #1개의 data가 784로 갖고있다. 여러개의 data=>none,784\n",
    "mytestimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytestimg1 = 1 - pixel1[:,:,0]/255 \n",
    "#검정색:1 , 흰색:0\n",
    "mytestimg1 = mytestimg1.reshape(1,784) #1개의 data가 784로 갖고있다. 여러개의 data=>none,784\n",
    "mytestimg1.shape\n",
    "# mytest=[mytestimg,mytestimg1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 168.853983523629\n",
      "Epoch: 2, Cost: 41.370346309488525\n",
      "Epoch: 3, Cost: 25.15873206875541\n",
      "Epoch: 4, Cost: 17.950760260495283\n",
      "Epoch: 5, Cost: 12.279062504226514\n",
      "Epoch: 6, Cost: 9.767962709407925\n",
      "Epoch: 7, Cost: 6.971358753590198\n",
      "Epoch: 8, Cost: 5.431878322108264\n",
      "Epoch: 9, Cost: 4.081774345923506\n",
      "Epoch: 10, Cost: 3.0603869925576506\n",
      "Epoch: 11, Cost: 2.3057003147041764\n",
      "Epoch: 12, Cost: 1.8279515336175562\n",
      "Epoch: 13, Cost: 1.3086358341714848\n",
      "Epoch: 14, Cost: 1.0216321512167188\n",
      "Epoch: 15, Cost: 0.8356333201842875\n",
      "Epoch: 16, Cost: 0.728710834102938\n",
      "Epoch: 17, Cost: 0.688880172607471\n",
      "Epoch: 18, Cost: 0.5042375190398534\n",
      "Epoch: 19, Cost: 0.4979157874714035\n",
      "Epoch: 20, Cost: 0.3800074108346984\n",
      "Learning Finish\n",
      "Accuracy of train data: 0.9953454732894897\n",
      "Accuracy of test data: 0.9527999758720398\n",
      "Prediction of 9: [[-552.9031    590.6263   1043.2035   1158.5989   1383.2513   -409.04852\n",
      "   567.03204  1269.8812    122.642586  666.1975  ]]\n",
      "Prediction of 3: [[-283.3379  1165.9186  2341.6687  2265.061   -520.74023 1353.6487\n",
      "   927.2744   613.1248   881.6318   115.4762 ]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 20 #epochs DL 용어\n",
    "batch_size = 100\n",
    "\n",
    "# tf.set_random_seed(100)\n",
    "X = tf.placeholder(tf.float32,[None,784]) #image (28x28) ->일렬로 784 한번에 받아들이기\n",
    "Y = tf.placeholder(tf.float32,[None,10]) #Class : 10개 0~9\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256])) \n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256])) \n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "# hypothesis = tf.nn.relu(tf.matmul(L2,W3) + b3) #회귀식 H(x) \n",
    "#relu~ (0보다 작은값을 지워버림=>음수값은 0으로 fitting)&activate function의 상한선을 없애버림\n",
    "hypothesis = tf.matmul(L2,W3) + b3 #회귀식 H(x) \n",
    "\n",
    "\n",
    "#softmax , 평균값()\n",
    "#(0,1)의 값 -> cross_entropy사용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "\n",
    "\n",
    "#train \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #epochs만큼 데이터를 학습시킴\n",
    "    #train_size를 batch size로 나눔\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        #batch size만큼 data를 넣어줌\n",
    "        #batch ~ 전체 데이터 크기가 커서 일부에 가중치를 둔다 - batch 개수만큼 가중치를 구함\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict=feed_dict)\n",
    "            #c, _ 가 뭐야? c:cost , _ :optimizer는 안쓰는 data이기 때문에 _를 씀\n",
    "            avg_cost += c / total_batch\n",
    "            #data를 나눠서 가져왔기때문에 cost를 평균값으로 함\n",
    "        #for - batch => 데이터 전체를 한 번 학습시킴\n",
    "    #for - epoch => 데이터 전체를(~fully scan) epoch번 학습시킴. 왜0o0?\n",
    "    #내가 갖고있는 데이터에 fitting을 epoch번 한다. epoch ↑ ~ overfitting(;overfitting=data에만 최적화/ 여기에서의 overfitting은\n",
    "    #최적화된 값을 찾을 때 까지 최대한 학습시키겠다.의 의미로 쓰임(overfitting 될수록 test accuracy 감소)\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch+1, avg_cost))\n",
    "    print(\"Learning Finish\")\n",
    "    \n",
    "    #accuracy - test data\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1)) #axis=1차원\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도 = 숫자값(0,1)~T/F 으로 평균 \n",
    "    print(\"Accuracy of train data: {}\".format(sess.run(accuracy, feed_dict={X:mnist.train.images, Y:mnist.train.labels})))\n",
    "    print(\"Accuracy of test data: {}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})))\n",
    "    print(\"Prediction of 9: {}\".format(sess.run(hypothesis, feed_dict={X:mytestimg}))) # Y:[[0,0,0,0,0,0,0,0,0,1]]})))\n",
    "    print(\"Prediction of 3: {}\".format(sess.run(hypothesis, feed_dict={X:mytestimg1})))#  Y:[[0,0,0,1,0,0,0,0,0,0]]})))\n",
    "#     pred = sess.run(hypothesis,feed_dict={X:mytestimg})\n",
    "#     print(\"Prediction of my data: {}\".format(np.max(pred==np.max(pred))))\n",
    "#     Y:[[0,0,0,0,0,0,0,0,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
