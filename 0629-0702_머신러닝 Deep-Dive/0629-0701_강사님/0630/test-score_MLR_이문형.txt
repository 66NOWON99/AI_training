import pandas as pd

data = pd.read_csv("./data-01-test-score.csv", sep=",", names = ['x_1', 'x_2', 'x_3', 'y'])
data.head

# x 피쳐에 bias 없는 경우
x_data = data[["x_1","x_2","x_3"]]
print(x_data.shape)
y_data = data["y"]
print(y_data.shape)

with tf.Graph().as_default():
    x = tf.placeholder(tf.float32, shape=[None,3]) #예제의 갯수는 상관없음 x feature 갯수만 3개로 고정
    y = tf.placeholder(tf.float32, shape=None)
    
    w = tf.Variable([[0,0,0]], dtype=tf.float32, name='weight') #가중치,오차 초기값 0
    b = tf.Variable(0, dtype=tf.float32, name='bias')
    
    y_hat = tf.matmul(w, tf.transpose(x)) + b
    
    loss = tf.reduce_mean(tf.square(y - y_hat)) #잔차 제곱의 평균, 학습되는 기준이므로 loss function이 굉장히 중요함
    
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00005)

    #이 파라미터 값으로는 [array([[0.35595763, 0.5407377 , 1.1615297 ]], dtype=float32), -3.701953]이 근사 최적해임

    # MLR의 hyper parameter = learning rate, epoch
    train = optimizer.minimize(loss)
    
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        sess.run(init)
        for step in range(1000000): #epoch = 100000
            sess.run(train, feed_dict={x:x_data, y:y_data})
            if(step % 50000 == 0):
                print(step, sess.run([w, b]))
        print(step, sess.run([w, b]))