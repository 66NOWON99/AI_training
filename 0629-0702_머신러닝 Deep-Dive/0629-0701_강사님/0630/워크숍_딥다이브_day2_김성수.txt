##데이터 뽑기
data = np.loadtxt("data-03-diabetes.csv",delimiter=",",dtype=np.float32)
x_data = data[:,:8]
y_data = data[:,8]
y_data= y_data[:,np.newaxis]

################################
##이상치 찾기

#텐서플로우 플랫폼에서 연산을 수행할 수 있는 그릇을 만들어준다
#None은 행의 갯수를 몰라서 비워둠
X = tf.placeholder(tf.float32, shape=[None, 8])
Y = tf.placeholder(tf.float32, shape=[None, 1])

W = tf.Variable(tf.random_normal([8,1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

#계단 함수 (x=0)이라면 연속이 되지 않으므로 미분이 불가능
#이렇게 미분이 되지 않는 지점에서 사용되는 것이 SIgmoid함수
#계단형식의 함수를 미분이 가능하도록 곡선화를 해주는 함수
#이때 기울기는 계단형식 그래프와 같아짐
hypothesis = tf.sigmoid(tf.matmul(X,W) + b) #1.0 / (1 + tf.exp(-(tf.matmul(X,W) + b)))
#cost에 엔트로피를 적용
#적용 이유 :
#시그모이드 함수는 0과 1의 사이의 값이다, 이를 제곱하게 되면
#차이를 크게 두어 원하는 값에 도달하는 점진하강을 못하고 0과 1의 값이여서 오히려 더 작게 됨
#엔트로피는 맞다면 0에 가까운 값에 도달하고, 틀리면 무한대에 가까운 값에 도달해서 엔트로피를 사용
cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))


train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
#Y값이 0.5 이상이면 1이 될확률이 더 높기 떄문에 직관적으로 예측가능
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
#예측값과 기대 값의 최소치를 구하여 이상치를 결정한다
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())#텐서플로우 초기화
    
    for step in range(10001):
        cost_val, _ = sess.run([cost, train], feed_dict={X:x_data, Y:y_data})
        if step % 200 == 0:
            print(step, cost_val)
    
    h, c, a = sess.run([hypothesis, predicted, accuracy], 
                       feed_dict={X:x_data, Y:y_data})
    print("\nHypothesis: ", h,
          "\nPredicted: ", c,
          "\nAccuracy: ", a)