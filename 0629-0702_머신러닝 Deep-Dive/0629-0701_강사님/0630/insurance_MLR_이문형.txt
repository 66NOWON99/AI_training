data_2 = pd.read_csv("./insurance.csv", sep=",", header=0)
data_2.head

data_2.dtypes

data_2_rev = pd.get_dummies(data_2)
data_2_rev.head

data_2_rev.dtypes

def feture_scaling(df, scaling_strategy="min-max", column=None):
    if column == None:
        column = [column_name for column_name in df.columns]
    for column_name in column:
        if scaling_strategy == "min-max":
            df[column_name] = ( df[column_name] - df[column_name].min() ) /\
                            (df[column_name].max() - df[column_name].min()) 
        elif scaling_strategy == "z-score":
            df[column_name] = ( df[column_name] - \
                               df[column_name].mean() ) /\
                            (df[column_name].std() )
    return df

feture_scaling(data_2_rev,column=["age","bmi","children","charges","sex_female","sex_male","smoker_no","smoker_yes","region_northeast","region_northwest","region_southeast","region_southwest"])
data_2_rev.head

# x 피쳐에 bias 없는 경우
x_data = data_2_rev[["age","bmi","children","sex_female","sex_male","smoker_no","smoker_yes","region_northeast","region_northwest","region_southeast","region_southwest"]]
print(x_data.shape)
y_data = data_2_rev["charges"]
print(y_data.shape)

with tf.Graph().as_default():
    tf.set_random_seed(777) # seed 값 고정, seed 값 변경하면서 초기 starting point를 찾아야 함.

    x = tf.placeholder(tf.float32, shape=[None,11]) #예제의 갯수는 상관없음 x feature 갯수만 3개로 고정
    y = tf.placeholder(tf.float32, shape=None)
    
    w = tf.Variable([[0,0,0,0,0,0,0,0,0,0,0]], dtype=tf.float32, name='weight') #가중치,오차 초기값 0
    b = tf.Variable(0, dtype=tf.float32, name='bias')
    
    y_hat = tf.matmul(w, tf.transpose(x)) + b
    
    loss = tf.reduce_mean(tf.square(y - y_hat)) #잔차 제곱의 평균, 학습되는 기준이므로 loss function이 굉장히 중요함
    
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05) #1e-5
    
    #이 파라미터 값으로는 0.009300222 [array([[ 0.1885989 ,  0.20124339,  0.03794948,  0.0303045 ,  0.02820851, -0.1610755 ,  0.21959633,  0.02399818,  0.01836418,  0.00747749, 0.00867394]], dtype=float32), 0.05851283]가 최적해임.
    # MLR의 hyper parameter = learning rate, epoch
    
    train = optimizer.minimize(loss)
    
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        sess.run(init)
        cost_history = []
        for step in range(100000): #epoch = 100000
            cost, hypothesis, _ = sess.run([loss, y_hat, train], feed_dict={x:x_data, y:y_data}) # cost 값도 출력하게함, train은 필요없으니 _로 선언
            cost_history.append(cost)
            if(step % 5000 == 0):
                print(step, cost, sess.run([w, b]))
        print(step, cost, sess.run([w, b]))
        
        # print("Your score will be ", sess.run(y_hat, feed_dict={x:[[60,70,110],[90,100,80]]}))
        
        plt.plot(cost_history)
        plt.show()