import tensorflow as tf
import numpy as np

def loadDataSet(fileName, sep='\t'):      #general function to parse tab -delimited floats
    numFeat = len(open(fileName).readline().split(sep)) - 1 #get number of fields 
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr =[]
        curLine = line.strip().split(sep)
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat,labelMat

xArr, yArr=loadDataSet("data-03-diabetes.csv", ',') 
# , 로 구분된 데이터를 읽어옴
x_data = np.array(xArr)
y_data = np.array(yArr)
y_data = y_data[:,np.newaxis] 


X = tf.placeholder(tf.float32, shape=[None,8])
Y = tf.placeholder(tf.float32, shape=[None,1])

W = tf.Variable(tf.random_normal([8,1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')
#weight bias 변수 생성

hypothesis = tf.sigmoid(tf.matmul(X,W)+b)
#시그모이드 함수 가정
#1.0 / (1+ np.exp(-(tf.matmul(X,W)+b)))

cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))
#decision tree 에서 entropy 계산 Y*tf.log(hypothesis)+(1-Y)*tf.log(1

train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
#오차가 작은 방향으로 학습 시킴, learning rate 간격씩 이동
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
#sigmoid 함수에서 예상한 값 0.5이상이면 1
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32))
#equal = 같으면 1 다르면 0 , casting 
#reduce_mean = 모든 차원지우고 하나의 평균 산출

with tf.Session() as sess: #tf 메모리에 올림
    sess.run(tf.global_variables_initializer())
    #initialize 하고 run
    for step in range(10001):
        cost_val, _ = sess.run([cost,train],feed_dict={X:x_data,Y:y_data})
        if step % 200 == 0: #200번에 한번씩 프린트
            print(step, cost_val)
            
    h,c,a = sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data})
    print('\nHypothesis: ',h,"\nPredicted: ",c,'\nAccuracy: ', a)
